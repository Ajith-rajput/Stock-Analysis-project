{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPSw5fxVtti9C+nSkh4/JRf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ajith-rajput/Stock-Analysis-project/blob/main/Stockanalysis_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUVwqzDDefJr",
        "outputId": "0531e036-8f04-4883-b08b-c9209df5453f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.0.tar.gz (316.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.0-py2.py3-none-any.whl size=317425344 sha256=504f31ebdd5fefa3ba85481b4c1b96290f05226728f6b61ee36b3962225034fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/41/4e/10/c2cf2467f71c678cfc8a6b9ac9241e5e44a01940da8fbb17fc\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark py4j"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "import os\n",
        "\n",
        "def initialize_spark():\n",
        "    return SparkSession.builder.appName(\"StockAnalysisProgram\").getOrCreate()\n",
        "\n",
        "def load_csv(spark, file_path):\n",
        "    return spark.read.option(\"header\", \"true\").csv(file_path)\n",
        "\n",
        "def add_symbol_column(df, file_name):\n",
        "    return df.withColumn(\"Symbol\", F.lit(os.path.splitext(file_name)[0]))\n",
        "\n",
        "def generate_summary_report(df):\n",
        "    return df.groupBy(\"Sector\").agg(\n",
        "        F.avg(\"open\").alias(\"Avg Open Price\"),\n",
        "        F.avg(\"close\").alias(\"Avg Close Price\"),\n",
        "        F.max(\"high\").alias(\"Max High Price\"),\n",
        "        F.min(\"low\").alias(\"Min Low Price\"),\n",
        "        F.avg(\"volume\").alias(\"Avg Volume\")\n",
        "    )\n",
        "\n",
        "# Define the folder path\n",
        "folder_path = \"/content/Data_source\" # Replace with path\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = initialize_spark()\n",
        "\n",
        "# Get list of file paths using list comprehension\n",
        "file_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, file_name))]\n"
      ],
      "metadata": {
        "id": "rhcOm5vDekfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV files into DataFrames and add a column\n",
        "main_df = None\n",
        "dfs = []\n",
        "\n",
        "for file_path in file_paths:\n",
        "    file_name = os.path.basename(file_path)\n",
        "\n",
        "    # Set \"symbol_metadata.csv\" as default main_df assuming it has the details of all company\n",
        "    if file_name == \"symbol_metadata.csv\":\n",
        "        main_df = load_csv(spark, file_path)\n",
        "    else:\n",
        "        df = load_csv(spark, file_path)\n",
        "        df = add_symbol_column(df, file_name)\n",
        "        dfs.append(df)\n",
        "\n",
        "for df in dfs:\n",
        "    common_column = \"Symbol\"  # common column if needed can be changed\n",
        "    joined_df = main_df.join(df, common_column, \"inner\")\n",
        "    dfs[dfs.index(df)] = joined_df\n",
        "\n",
        "# Initialize merged_df with the first DataFrame\n",
        "merged_df = dfs[0]\n",
        "\n",
        "# Union all DataFrames in dfs_to_merge\n",
        "for df in dfs[1:]:\n",
        "     merged_df = merged_df.unionAll(df)"
      ],
      "metadata": {
        "id": "QWNTtCjXe5eL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate summary reports ALL TIME\n",
        "summary_report = generate_summary_report(merged_df)\n",
        "\n",
        "# Show the summary report\n",
        "summary_report.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtFszVsde9wp",
        "outputId": "f76f4025-ebe9-4af5-df99-33d6670e79f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+------------------+------------------+--------------+-------------+-------------------+\n",
            "|              Sector|    Avg Open Price|   Avg Close Price|Max High Price|Min Low Price|         Avg Volume|\n",
            "+--------------------+------------------+------------------+--------------+-------------+-------------------+\n",
            "|ENERGY & TRANSPOR...| 17.56280019904608|17.556030487850713|       99.9900|       0.1980|  4669611.431967552|\n",
            "|             FINANCE|19.604653790169127|19.610372925764274|        9.9900|      10.0000| 10065.089239726794|\n",
            "|       LIFE SCIENCES| 45.30218615437486|45.309679145671055|       99.9300|      10.2900| 1772380.0983737975|\n",
            "|       MANUFACTURING|32.826966348273345| 32.84054371785455|       92.3200|      10.0850|  5396276.952240999|\n",
            "|REAL ESTATE & CON...|10.176505082417581|10.168689285714288|        9.9900|      10.0000|  212976.1813186813|\n",
            "|          TECHNOLOGY| 72.61458867796587| 72.61408919774018|       99.9900|       0.0900|1.149873897677966E7|\n",
            "|    TRADE & SERVICES|58.672824213892305| 58.68767543080501|       99.9000|     100.0000| 2475920.9017587495|\n",
            "+--------------------+------------------+------------------+--------------+-------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accept user input for start date, end date, and sectors\n",
        "start_date = input(\"Enter start date (yyyy-mm-dd): \")\n",
        "end_date = input(\"Enter end date (yyyy-mm-dd): \")\n",
        "sectors = [sector.strip().upper() for sector in input(\"Enter sectors (comma-separated, e.g., Sector1,Sector2): \").split(\",\")]\n",
        "\n",
        "# Filter the DataFrame based on the user-provided date range and sectors\n",
        "filtered_df = merged_df.filter((merged_df[\"timestamp\"] >= start_date) &\n",
        "                               (merged_df[\"timestamp\"] <= end_date) &\n",
        "                               (merged_df[\"Sector\"].isin(sectors)))\n",
        "\n",
        "# Generate summary reports for the selected sectors\n",
        "summary_report_for_given_time = generate_summary_report(filtered_df)\n",
        "\n",
        "# Show the summary report\n",
        "summary_report_for_given_time.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "valev6dFfCpf",
        "outputId": "38b256d4-4273-42d0-bd43-2eeff6667052"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter start date (yyyy-mm-dd): 2022-01-01\n",
            "Enter end date (yyyy-mm-dd): 2022-02-20\n",
            "Enter sectors (comma-separated, e.g., Sector1,Sector2): technology\n",
            "+----------+-----------------+-----------------+--------------+-------------+-------------------+\n",
            "|    Sector|   Avg Open Price|  Avg Close Price|Max High Price|Min Low Price|         Avg Volume|\n",
            "+----------+-----------------+-----------------+--------------+-------------+-------------------+\n",
            "|TECHNOLOGY|81.04880147058823|80.94974264705881|       96.8200|     154.7000|2.531880049264706E7|\n",
            "+----------+-----------------+-----------------+--------------+-------------+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accept user input for start date, end date, and sector\n",
        "start_date = input(\"Enter start date (yyyy-mm-dd): \")\n",
        "end_date = input(\"Enter end date (yyyy-mm-dd): \")\n",
        "sector = input(\"Enter sector: \").strip().upper()  # Removing trailing spaces\n",
        "\n",
        "# Filter the DataFrame based on the user-provided sector\n",
        "filtered_df_symbol = merged_df.filter((merged_df[\"Sector\"] == sector))\n",
        "\n",
        "# Generate summary reports for all symbols in the selected sector\n",
        "summary_report_symbol = filtered_df_symbol.groupBy(\"Symbol\", \"Name\").agg(\n",
        "    F.avg(\"open\").alias(\"Avg Open Price\"),\n",
        "    F.avg(\"close\").alias(\"Avg Close Price\"),\n",
        "    F.max(\"high\").alias(\"Max High Price\"),\n",
        "    F.min(\"low\").alias(\"Min Low Price\"),\n",
        "    F.avg(\"volume\").alias(\"Avg Volume\")\n",
        ")\n",
        "\n",
        "# Show the summary report for symbols in the selected sector\n",
        "summary_report_symbol.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k9IJ7p8EfK7s",
        "outputId": "3eea8183-916a-4de1-e8f5-9fdcea20e82d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter start date (yyyy-mm-dd): 2022-01-01\n",
            "Enter end date (yyyy-mm-dd): 2022-02-20\n",
            "Enter sector: technology\n",
            "+------+--------------------+------------------+------------------+--------------+-------------+--------------------+\n",
            "|Symbol|                Name|    Avg Open Price|   Avg Close Price|Max High Price|Min Low Price|          Avg Volume|\n",
            "+------+--------------------+------------------+------------------+--------------+-------------+--------------------+\n",
            "|  AAPL|           Apple Inc| 174.2094461716107|174.19959467045692|       99.9900|     100.0000|2.9121742522472907E7|\n",
            "|  DELL|Dell Technologies...| 65.07967503075035| 65.10261574415749|       99.9000|     100.1100|   2790933.971709717|\n",
            "|  NTAP|          NetApp Inc|40.921400248711926| 40.92299943151524|       99.0000|      10.0000|  5492976.7871735655|\n",
            "|  QMCO| Quantum Corporation|3.8011919168591386|3.8045603126665557|        9.9900|       0.0900|  1139171.3583229703|\n",
            "+------+--------------------+------------------+------------------+--------------+-------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}